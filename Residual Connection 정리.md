### **Residual Connection 개념**

- **목적**: 신경망이 깊어질수록 발생하는 **그래디언트 소실** 문제를 완화하고, 모델이 이전 층의 정보를 더 잘 학습하도록 도움.
- 방식
  - 일반적으로 이전 층의 출력을 그대로 더하거나 연결(`Concatenate`)하여 다음 층의 입력으로 사용.
  - **더하기(`Add`)** 연산을 사용하면 원래 입력의 차원이 동일해야 함.
  - **연결(`Concatenate`)** 연산을 사용하면 차원이 달라도 병합 가능.

------

### **Residual Connection 적용**

1. **Input 데이터를 직접 연결**:
   - 이전 출력(`x`)에 원래 입력(`Input_data`)를 연결.
   - "이전 값 + 현재 값"이 되는 형태.
2. **이전 값을 연결**:
   - **가장 최근의 Residual 값을 재활용**하여 새로운 Residual Connection을 쌓아야 함.
   - 각 Residual은 이전 출력(`residual`)을 기준으로 설계.

**`Concatenate` 사용 이유**:

- 입력 차원이 다를 수 있으므로 `Concatenate`를 사용하여 병합.
- 예를 들어, `Dense` 레이어 뒤 출력은 원래 입력 차원과 다르기 때문.

**Residual Connection의 대상**:

- Residual Connection은 **이전 Residual 값(`residual`)과 현재 값(`x`)**을 연결하는 방식으로 쌓음.

**모델 안정성**:

- Residual Connection은 정보를 그대로 전달하므로 모델 학습이 안정적이고, 과도한 변형을 방지.



### **1. 층의 깊이 결정**

**시계열 데이터 예측 모델에서 층의 깊이와 구조를 정하는 기준:**

1. **데이터 크기 및 차원**
   - 입력 데이터 크기 `(79735, 618)`는 상당히 큰 편이며, **618개의 피처**는 충분히 복잡한 입력 공간을 나타냅니다.
   - 따라서 모델이 충분히 깊거나 넓어야 데이터를 잘 학습할 수 있습니다.
2. **문제의 복잡도**
   - 1시간 뒤 안개의 상태를 예측하는 문제는 비선형 관계를 잘 학습해야 할 수 있으므로, 기본적으로 **2~4개의 Residual Block**을 사용하는 것이 적합합니다.
   - 필요하면 점진적으로 층을 추가하여 성능 변화를 확인합니다.
3. **ResNet 스타일 설계**
   - Residual Connection은 층이 깊어져도 그래디언트 소실 문제를 완화하므로, 층을 더 쌓아도 비교적 안정적입니다.
   - 다만, 지나치게 많은 파라미터가 추가되면 과적합 가능성이 높아지므로 정규화(Dropout)와 조기 종료(Early Stopping) 등을 함께 고려하세요.